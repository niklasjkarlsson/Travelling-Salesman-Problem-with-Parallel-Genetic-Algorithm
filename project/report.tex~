\documentclass[12pt]{article}
\usepackage{graphicx}
\usepackage{listings}
\lstset{
  numbers=left
}

\begin{document}
\section*{MATR326 Tools of high performance computing 2022}
Exercise 1 \\
Author: Allan Turing\\
Student id: 1234\\

\section{Problem 1}
%Execute the following code and report the results
When simply executing the code, it returned an error asking for a size input.
Therefore, from this point on I always supply a size for it.

Based on the output given a size, I believe the code creates two random matrices and computes their multiplication. Furthermore, when executing the code multiple times we see that the values in the matrices seem to be always the same. This points to the fact that the function used for the random generator uses always the same seed.
An example of such a run can be seen in Figure~\ref{fig:example}.
For it I used an input size $2$ for readability.

	\begin{figure}[h]
		\centering
		\includegraphics[width=8cm]{img/ex1exec}
		\label{fig:example}
		\caption{Program output with input $2$}
	\end{figure}
\section{Problem 2}
When studying the code, I identified two sections of code that most of the runtime concentrated at.
These are the nested for loops, which can be easily parallelized by using the OPENMP~\cite{openmp45} library.
They can be seen in Code~\ref{code:assign} and Code~\ref{code:multi}.
In line $1$ of both codes we can see the code addtion for the parallelization of the loops.
This is a compiler instruction, meaning that the first for of the loop will be run in parallel.
The private intruction is followed by the name of the variables which are individual for each thread.
Firstprivate is similar. 
However, the value of the variable will hold the same value as it was initialized before the thread creation.
The remaining variables will be initialized inside each thread.
By default, the number of workers should be the same as the number of cores in the machine.
In my case, this should be $4$.

%Write a program that solves the following problem

\begin{center}
	\framebox{\lstinputlisting[caption={Matrices assignment},captionpos=b, language=C, firstline=60, lastline=69]{code/best.c}}
	\label{code:assign}
\end{center}

\begin{center}
	\framebox{\lstinputlisting[caption={Matrix multiplication},captionpos=b, language=C, firstline=77, lastline=91]{code/best.c}}
	\label{code:multi}
\end{center}

\section{Problem 3}
%Compare the runtimes of these two codes. Explain the possible reasons for the results.
In order to track the runtime of the code, I used the $gettimeofday$ function.
It returns CPU time in a precision up to miliseconds, which seems to be enough for our case.
That is, by choosing a large enough size for the matrices the time the program takes for its execution far superceeds this precision.

Since the OPENMP library requires to be linked in compilation time, I have added a new line to the makefile.
Now we can compile both parallelized and sequential versions of the code at the same time.

	\begin{center}
		\framebox{\lstinputlisting[language=Bash]{code/makefile}}
	\end{center}

After commenting the lines as per instructed in the original code, I created this bash script to execute each program with different sizes.
The input sizes start at $1$ and end at $2000$.
The increase was set to $100$, meaning $20$ executions in total.
	\begin{center}
		\framebox{\lstinputlisting[language=Bash]{code/run.sh}}
	\end{center}

Since the time is collected inside the program, I outputed it once the measurements are done.
Each program outputs to their respective data file, parallel.dat and sequential.dat.
I wrote a small Python script to plot the data using the Matplotlib~\cite{matplot} library.
The plotted results can be seen in Figure~\ref{fig:runtimes}.
The blue line is the sequential code, whereas the orange one is the parallel version.
On the Y axis we have the runtime in seconds, wereas the X axis is a simple identifier for each execution.

	\begin{figure}[h]
		\centering
		\includegraphics[scale=0.8]{img/runtimes}
		\label{fig:runtimes}
		\caption{Runtime comparison from size 1 to 2000}
	\end{figure}
	
We can see in the figure that both codes have a similar exponential behavior. 
This is due to the exponential nature of the matrix multiplication problem it is solving.
Still, the sequential program (for the sizes explored) seems to consistenly take much longer to execute then the parallel one.
However, sequential program seems to have a speedup of only $2$, despite my computer having $4$ cores.
When studying the specification of my processor, I found out that it does, in fact, have only $2$ physical cores.
The extra $2$ cores are virtual cores due to Intel's Hyper Threading\footnote{Info at https://ark.intel.com/content/www/us/en/ark/products/54619/intel-core-i52537m-processor-3m-cache-up-to-2-30-ghz.html}.

\bibliographystyle{alpha}
\bibliography{references}

\end{document}